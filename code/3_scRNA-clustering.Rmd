---
title: "scRNA-seq Tutorial 3"
author: "Suparna"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 12, fig.height = 12, message = FALSE)
```

# Single Cell RNA-seq Workshop - Day 2: Clustering Cells Based on Top Principal Components (Metagenes)

The goal of clustering is to generate cell type-specific clusters and use known cell type marker genes to determine the identities of the clusters. Clustering also determines whether clusters represent true cell types or biological / technical variation, such as clusters of cells in the S phase of the cell cycle, clusters of specific batches, or cells with high mitochondrial content.

**Challenges:**

* Identifying poor quality clusters that may be due to uninteresting biological or technical variation
* Identifying the cell types of each cluster
* Maintaining patience as this can be a highly iterative process between clustering and marker identification (sometimes even going back to the QC filtering)

**Recommendations:**

* Have a good idea of your expectations for the cell types to be present prior to performing the clustering. Know whether you expect cell types of low complexity or higher mitochondrial content AND whether the cells are differentiating.

* If you have more than one condition, it’s often helpful to perform integration to align the cells.

* Regress out number of UMIs (by default with `SCTransform`), mitochondrial content, and cell cycle, if needed and appropriate for experiment, so as to not drive clustering.

* Identify any junk clusters for removal or re-visit QC filtering. Possible junk clusters could include those with high mitochondrial content and low UMIs/genes. If comprised of a lot of cells, then may be helpful to go back to QC to filter out, then re-integrate/cluster.

* If not detecting all cell types as separate clusters, try changing the resolution or the number of PCs used for clustering.

## Load Libraries & Data
```{r}
library(Seurat)
library(tidyverse)
library(RCurl)
library(cowplot)

# Load data
seurat_integrated <- readRDS(file = "results/integrated_seurat.rds")
```

## Identify significant PCs

To overcome the extensive technical noise in the expression of any single gene for scRNA-seq data, Seurat assigns cells to clusters based on their PCA scores derived from the expression of the integrated most variable genes, with each PC essentially representing a “metagene” that combines information across a correlated gene set.

Determining how many PCs to include in the clustering step is therefore important to ensure that we are capturing the majority of the variation, or cell types, present in our dataset.

It is useful to explore the PCs prior to deciding which PCs to include for the downstream clustering analysis.

### Explore PCs - Heatmap

Used to visualize the most variant genes for select PCs with the genes and cells ordered by PCA scores. We look at the PCs and determine whether the genes driving them make sense for differentiating the different cell types.

The `cells` argument specifies the number of cells with the most negative or postive PCA scores to use for the plotting. We are looking for a PC where the heatmap starts to look more “fuzzy”, i.e. where the distinctions between the groups of genes is not so distinct.

```{r}
# Explore heatmap of PCs
DimHeatmap(seurat_integrated, 
           dims = 1:9, 
           cells = 500, 
           balanced = TRUE)
```


This method can be slow and hard to visualize individual genes if we would like to explore a large number of PCs. In the same vein and to explore a large number of PCs, we could print out the top 10 (or more) positive and negative genes by PCA scores driving the PCs.

```{r}
# Printing out the most variable genes driving PCs
print(x = seurat_integrated[["pca"]], 
      dims = 1:10, 
      nfeatures = 5)
```

### Explore PCs - Elbow Plot

The elbow plot is helpful when determining how many PCs we need to capture the majority of the variation in the data. The elbow plot visualizes the standard deviation of each PC. Where the elbow appears is usually the threshold for identifying the majority of the variation. However, this method can be a bit subjective about where the elbow is located.

Let’s draw an elbow plot using the top 40 PCs:

```{r}
# Plot the elbow plot
ElbowPlot(object = seurat_integrated, 
          ndims = 40)
```

Based on this plot, we could roughly determine the majority of the variation by where the elbow occurs around PC8 - PC10, or one could argue that it should be when the data points start to get close to the X-axis, PC30 or so.

While this gives us a good rough idea of the number of PCs needed to be included, a more quantitative approach may be a bit more reliable. We can calculate where the principal components start to elbow by taking the larger value of:

1. The point where the principal components only contribute 5% of standard deviation and the principal components cumulatively contribute 90% of the standard deviation.

2. The point where the percent change in variation between the consecutive PCs is less than 0.1%.


Calculating the first metric:

```{r}
# Determine percent of variation associated with each PC
pct <- seurat_integrated[["pca"]]@stdev / sum(seurat_integrated[["pca"]]@stdev) * 100

# Calculate cumulative percents for each PC
cumu <- cumsum(pct)

# Determine which PC exhibits cumulative percent greater than 90% and % variation associated with the PC as less than 5
co1 <- which(cumu > 90 & pct < 5)[1]

co1
```


The first metric returns PC42 as the PC matching these requirements. Let’s check the second metric, which identifies the PC where the percent change in variation between consecutive PCs is less than 0.1%:

```{r}
# Determine the difference between variation of PC and subsequent PC
co2 <- sort(which((pct[1:length(pct) - 1] - pct[2:length(pct)]) > 0.1), decreasing = T)[1] + 1

# last point where change of % of variation is more than 0.1%.
co2
```

This second metric returns PC18. Usually, we would choose the minimum of these two metrics as the PCs covering the majority of the variation in the data.

```{r}
# Minimum of the two calculation
pcs <- min(co1, co2)

pcs
```

Based on these metrics, for the clustering of cells in Seurat we will use the first 18 PCs to generate the clusters. We can plot the elbow plot again and overlay the information determined using our metrics:

```{r}
# Create a dataframe with values
plot_df <- data.frame(pct = pct, 
           cumu = cumu, 
           rank = 1:length(pct))

# Elbow plot to visualize 
  ggplot(plot_df, aes(cumu, pct, label = rank, color = rank > pcs), size = 10) + 
  geom_text(size = 7) + 
  geom_vline(xintercept = 90, color = "grey") + 
  geom_hline(yintercept = min(pct[pct > 5]), color = "grey") +
  theme_bw() +
  theme(axis.text.x = element_text(size=rel(3), angle = 45, hjust = 1),
        axis.text.y = element_text(size=rel(4)),
        axis.title = element_text(size=rel(4)),
        plot.title = element_text(size=rel(4), hjust = 0.5, face = "bold"),
        legend.text = element_text(size=20),
        legend.title = element_text(size=20))
```


### Explore PCs - SCTransform

While the above 2 methods were used a lot more with older methods from Seurat for normalization and identification of variable genes, they are no longer as important as they used to be. This is because the `SCTransform` method is more accurate than older methods.

The older methods incorporated some technical sources of variation into some of the higher PCs, so selection of PCs was more important. `SCTransform` estimates the variance better and does not frequently include these sources of technical variation in the higher PCs.

In theory, with `SCTransform`, the more PCs we choose the more variation is accounted for when performing the clustering, but it takes a lot longer to perform the clustering. Therefore for this analysis, we will use the **first 40 PCs** to generate the clusters.

Seurat uses a graph-based clustering approach, which embeds cells in a graph structure, using a K-nearest neighbor (KNN) graph (by default), with edges drawn between cells with similar gene expression patterns. Then, it attempts to partition this graph into highly interconnected ‘quasi-cliques’ or ‘communities’ 

We will use the `FindClusters()` function to perform the graph-based clustering. The resolution is an important argument that sets the “granularity” of the downstream clustering and will need to be optimized for every individual experiment. For datasets of 3,000 - 5,000 cells, the resolution set between `0.4`-`1.4` generally yields good clustering. Increased resolution values lead to a greater number of clusters, which is often required for larger datasets.

The `FindClusters()` function allows us to enter a series of resolutions and will calculate the “granularity” of the clustering. This is very helpful for testing which resolution works for moving forward without having to run the function for each resolution.


```{r}
# Determine the K-nearest neighbor graph
seurat_integrated <- FindNeighbors(object = seurat_integrated, 
                                dims = 1:40)
                                
# Determine the clusters for various resolutions                                
seurat_integrated <- FindClusters(object = seurat_integrated,
                               resolution = c(0.4, 0.6, 0.8, 1.0, 1.4))
```


If we look at the metadata of our Seurat object(`seurat_integrated@meta.data`), there is a separate column for each of the different resolutions calculated.

```{r}
# Explore resolutions
View(seurat_integrated@meta.data)
```

To choose a resolution to start with, we often pick something in the middle of the range like 0.6 or 0.8. We will start with a resolution of 0.8 by assigning the identity of the clusters using the `Idents()` function.

You can visualize cell clusters using tSNE or UMAP. Both methods aim to place cells with similar local neighborhoods in high-dimensional space together in low-dimensional space. These methods will require you to input number of PCA dimentions to use for the visualization, we suggest using the same number of PCs as input to the clustering analysis. Here, we will proceed with the UMAP method for visualizing the clusters.

We already calculated UMAP using `RunUMAP()` in the previous lesson.

```{r}
# Assign identity of clusters
Idents(object = seurat_integrated) <- "integrated_snn_res.0.8"

# Plot the UMAP
DimPlot(seurat_integrated,
        reduction = "umap",
        label = TRUE,
        label.size = 6)
```

It is possible that there is some variability in the way your clusters look compared to the image in this lesson. In particular you may see a difference in the labeling of clusters. This is an unfortunate consequence of slight variations in the versions of packages (mostly Seurat dependencies).


## Changing the RData

Since my clusters look different from the ones in the lesson, I have to proceed using a different RData file.

Start by clearing all variables in the environment. Then load the new data:

```{r}
load("data/seurat_integrated.RData")
```

We will now continue with the 0.8 resolution to check the quality control metrics and known markers for the anticipated cell types. Plot the UMAP again to make sure your image now (or still) matches what you see in the lesson:

```{r}
# Assign identity of clusters
Idents(object = seurat_integrated) <- "integrated_snn_res.0.8"

# Plot the UMAP
DimPlot(seurat_integrated,
        reduction = "umap",
        label = TRUE,
        label.size = 6)
```


## Session Information
```{r}
sessionInfo()
```

